# 预训练模型分词实验

## 实验目的

中文分词指的是中文在基本文法上有其特殊性而存在的分词。中文分词是自然语言处理中文部分的基础，很多下游任务都需要基于分词的基础进行，分词的质量也一定程度上决定了下游任务的准确性。本次实验拟通过预训练算法，在中文数据集PKU 与 MSR上实现中分分词模型。主要的实验目的包括：

- 掌握中文分词（Tokenization）任务的基本解决思路。学习自然语言处理数据集的处理与词表的构建方法。
- 借助基于预训练的方法实现中文分词。掌握BERT等大模型基本结构与用法。学习超大模型微调的基本方法。
- 体会预训练模型训练过程的一些细节。
- 掌握评价分词好坏的评价指标。学会分析Bad Case以及产生的原因，并尝试寻找解决的思路与办法。

## 实验原理

中文分词是中文文本处理的一个基础步骤，也是中文人机自然语言交互的基础模块。不同于英文的是，中文句子中没有词的界限，因此在进行中文自然语言处理时，通常需要先进行分词，分词效果将直接影响词性、句法树等模块的效果。

在人机自然语言交互中，成熟的中文分词算法能够达到更好的自然语言处理效果，帮助计算机理解复杂的中文语言。在构建中文自然语言对话系统时，结合语言学不断优化，训练出了一套具有较好分词效果的算法模型，为机器更好地理解中文自然语言奠定了基础。

随着网络信息的急剧增长给人们搜索信息带来一定的困难，中文分词到底对搜索引擎有多大影响？对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。

次实验针对中文分词技术，基于中文的预训练模型Bert-base-Chinese进行下游任务的微调，通过在输出层对比Softmax输出和CRF输出的效果。即Bert base和Bert-CRF预训练模型方法来实现中文分词。

### BERT模型

BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的masked language model（MLM），以致能生成深度的双向语言表征。

该模型有以下主要优点：

1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。

2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。

### BERT+CRF模型

条件随机场（Conditional Random Field，CRF）是自然语言处理的基础模型，广泛应用于中文分词、命名实体识别、词性标注等标注场景。条件随机场是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫随机场，条件随机场可以用于不同的预测问题。条件随机场模型的分解式：
$$
p(\boldsymbol{s} \mid \boldsymbol{o})=\frac{1}{Z(\boldsymbol{o})} \prod_{c \in C} \exp \left(\phi_{c}\left(\boldsymbol{s}_{c}, \boldsymbol{o}\right)\right)=\frac{1}{Z(\boldsymbol{o})} \exp \left(\sum_{c \in C} \phi_{c}\left(\boldsymbol{s}_{c}, \boldsymbol{o}\right)\right)
$$
条件随机场模型的解码算法：
$$
\begin{aligned}
\boldsymbol{s}^{*} &=\underset{s}{\operatorname{argmax}} p(\boldsymbol{s} \mid \boldsymbol{o}) \\
&=\underset{s}{\operatorname{argmax}} \sum_{i} \phi_{c_{i}}\left(s_{i-1}, s_{i}, \boldsymbol{o}\right)
\end{aligned}
$$
其中Viterbi变量：
$$
\delta_{i}(s) \triangleq \max _{s_{1}, \ldots, S_{i-1}}\left[\sum_{k=1}^{i-1} \phi_{c_{k}}\left(s_{k-1}, s_{k}\right)+\phi_{c_{i}}\left(s_{i-1}, s\right)\right]
$$
条件随机场的特点：理论上较为完善的序列标记模型，无标记偏执问题；兼具判别模型和无向图模型的优点，特征设计灵活、无需要求特征独立；条件随机场模型训练代价大、复杂度高。

CRF是解决序列问题最终输出序列的传统算法模型，正常的序列标注采用SoftMax针对序列每个位置完成一次多分类，但CRF是针对整个序列选择出一条最优的序列分类结果，考虑了序列整体输出的特征。

因此，条件随机场CRF与深度学习结合，产生了BERT-CRF，在中文分词、命名实体识别、词性标注也取得不错的效果。

## 实验方案

### BERT模型分词

我们知道，中文分词的本质是一个序列标注问题，对于序列标注问题我们需要确定标记的类别。常用的标记方法采用“SBME”标注方法：“S”（Single）表示单个字作为词；“B”（Begin）表示一个词语的开始第一个字；“E”（End）表示一个词语结尾最后一个字；“M”（Middle）表示一个单词中间的那些字。这样“S”，“B”，“E”，“M”组成了标注序列的符号集合。针对BERT预训练模型进行中文分词的具体内容和步骤如下：

1. 构造BERT数据集，在输入的sentence开头加上“[CLS]”标识，结尾加上“[SEP]”标识，同时将两个标识符的索引加入词表字典中；除此之外，还需要将“[PAD]”占位字符添加到词表。

2. 根据生成的训练语料List，根据“SBME”标注方法，生成与之对应的标记List，根据训练数据集中语料空格分词的特性，以及“SBME”四种标签的含义，便可以构造出字符级别的跟语料库对应的标签语料。

3. 借助HuggingFace提供的开源Transformers库预训练模型下的“bert-base-chinese”中文BERT模型进行微调的实现。采用Torch DDP进行分布式多卡训练。

4. 划分训练集为训练和验证集，便于保存训练过程中最优的模型。同时设置训练的batch_size = 6，学习率lr = 0.00001，训练轮次epoches = 50。

5. 利用测试集测试模型的效果并对一些case进行分析。

### BERT+CRF模型分词

BERT+CRF模型在BERT的模型基础上，在输出的地方添加一个词表大小×词表大小的转移矩阵，注意这里的词表相对BERT需要多添加“\<start>”和“\<end>”字符。

在测试解码的过程中，则需要维特比算法来求解最佳的输出解码序列。

## 实验结果

### PKU数据集

在PKU数据集上的模型的几大评价指标如下表所示：

| 模型          | 准确率     | 召回率     | F1分数     |
| ------------- | ---------- | ---------- | ---------- |
| Uni-Gram      | 0.8550     | 0.9342     | 0.8928     |
| Uni-Gram+规则 | 0.9111     | 0.9496     | 0.9300     |
| HMM           | 0.7936     | 0.8090     | 0.8012     |
| CRF           | 0.9409     | 0.9396     | 0.9400     |
| Bi-LSTM       | 0.9248     | 0.9236     | 0.9240     |
| Bi-LSTM+CRF   | 0.9366     | 0.9354     | 0.9358     |
| **BERT**      | **0.9712** | **0.9635** | **0.9673** |
| **BERT-CRF**  | **0.9705** | **0.9619** | **0.9662** |
| jieba         | 0.8559     | 0.7896     | 0.8214     |
| pkuseg        | 0.9512     | 0.9224     | 0.9366     |
| THULAC        | 0.9287     | 0.9295     | 0.9291     |

### MSR数据集

在MSR数据集上的模型的几大评价指标如下表所示：

| 模型          | 准确率     | 召回率     | F1分数     |
| ------------- | ---------- | ---------- | ---------- |
| Uni-Gram      | 0.9119     | 0.9633     | 0.9369     |
| Uni-Gram+规则 | 0.9129     | 0.9634     | 0.9375     |
| HMM           | 0.7786     | 0.8189     | 0.7983     |
| CRF           | 0.9675     | 0.9676     | 0.9675     |
| Bi-LSTM       | 0.9624     | 0.9625     | 0.9624     |
| Bi-LSTM+CRF   | 0.9631     | 0.9632     | 0.9632     |
| **BERT**      | **0.9841** | **0.9817** | **0.9829** |
| **BERT-CRF**  | **0.9805** | **0.9787** | **0.9796** |
| jieba         | 0.8204     | 0.8145     | 0.8174     |
| pkuseg        | 0.8701     | 0.8894     | 0.8796     |
| THULAC        | 0.8428     | 0.8880     | 0.8648     |

## 实验分析

实验对传统算法，神经网络算法，预训练模型方法以及开源预训练，观察不同模型间的效果差异：

- 在PKU数据集上，在传统算法中，CRF的效果十分显著相比其他算法的效果更优，Bi-LSTM以及添加CRF层的效果比传统语言模型和jieba和THULAC开源分词库的效果好，但是比只有CRF和pkuseg的效果略差。BERT预训练模型的效果明显比其他方法的效果更优，原因可能是大模型能够在语义程度上理解任务，是的模型能够克服传统方法的弊端。但是，随着模型大小的增加，处理推理的速度也必然会有所下降。

- 在MSR数据集上，在传统算法中，CRF的效果仍然十分显著，相比其他算法的效果更优。但是在该数据集上Bi-LSTM以及添加CRF层的效果却优于列举出的所有开源分词工具以及传统算法。BERT等预训练模型的效果则显得更加突出，训练精度达到了很高的水平。